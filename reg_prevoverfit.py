# -*- coding: utf-8 -*-
"""reg_prevOverfit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IVHUdNbh4csRW55hegZqQ8eiIJ8F1zg7
"""

from keras.datasets import mnist

import numpy as np
import matplotlib.pyplot as plt

from keras import models, layers, Sequential, regularizers
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical

(X_train, y_train), (X_test, y_test)=mnist.load_data()

images=X_train[:3]
labels=y_train[:3]

for i in range (3):
  print("Digit in the image is" ,labels[i])
  plt.imshow(images[i])
  plt.show()

input_num_units = 784
X_train = X_train.reshape(60000, input_num_units)
X_test = X_test.reshape(10000, input_num_units)

y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

hid1_num_units=500
hid2_num_units=500
hid3_num_units=500
hid4_num_units=500
hid5_num_units=500
op_num_units=10

epochs=10
batch_size=128

model=Sequential([
    layers.Dense(hid1_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dense(hid2_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dense(hid3_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dense(hid4_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dense(hid5_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dense(op_num_units,input_dim=hid5_num_units,activation='relu'),
])

def runModel():
  model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])
  trained_model_5d=model.fit(X_train, y_train,epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test))

runModel()

model=Sequential([
    layers.Dense(hid1_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(hid2_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(hid3_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(hid4_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(hid5_num_units,input_dim=input_num_units,activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(op_num_units,input_dim=hid5_num_units,activation='relu'),
])

runModel()

model=Sequential([
    layers.Dense(hid1_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l1(0.0001)),
    layers.Dense(hid2_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l1(0.0001)),
    layers.Dense(hid3_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l1(0.0001)),
    layers.Dense(hid4_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l1(0.0001)),
    layers.Dense(hid5_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l1(0.0001)),
    layers.Dense(op_num_units,input_dim=hid5_num_units,activation='softmax'),
])

runModel()

model=Sequential([
    layers.Dense(hid1_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l2(0.0001)),
    layers.Dense(hid2_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l2(0.0001)),
    layers.Dense(hid3_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l2(0.0001)),
    layers.Dense(hid4_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l2(0.0001)),
    layers.Dense(hid5_num_units,input_dim=input_num_units,activation='relu',
    kernel_regularizer=regularizers.l2(0.0001)),
    layers.Dense(op_num_units,input_dim=hid5_num_units,activation='softmax'),
])

runModel()

EarlyStopping_Model=model.fit(X_train, y_train,epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_accuracy', patience=2)] )

"""#Data Augmentation"""

from keras.preprocessing.image import ImageDataGenerator


(X_train, y_train), (X_test, y_test) = mnist.load_data()
# image dimensions (assumed square)
image_size = X_train.shape[1]
input_size = image_size * image_size
# we train our network using float data
X_train = X_train.astype('float32') / 255
X_test = X_test.astype('float32') / 255

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)


batch_size = 10000

X_train = np.reshape(X_train, [-1, image_size, image_size, 1])

datagen = ImageDataGenerator(zca_whitening=True)
datagen.fit(X_train)

for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=batch_size):
  for i in range(0, 9):
            plt.subplot(330 + 1 + i)
            img=X_batch[i].reshape(28, 28)
            plt.imshow(img, cmap=plt.get_cmap('gray'))
  plt.show()
  break

X_batch = np.reshape(X_batch, [-1, image_size*image_size])

model.fit(X_batch,y_batch, epochs=10)

x_test = np.reshape(X_test, [-1, input_size])
scores = model.evaluate(x_test,
                        y_test,
                        batch_size=batch_size,
                        verbose=False)

print('Test loss:', scores[0])
print('Test accuracy: %0.1f%%' % (100 * scores[1]))